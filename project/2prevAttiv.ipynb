{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file dove viene ricercato il modello ottimale per prevedere se uno studente sarà attivo all' anno successivo o meno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import funzioni\n",
    "from funzioni import *\n",
    "\n",
    "import settings\n",
    "from settings import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================================================================================================================================================================\n",
    "#                                                                      OPERAZIONI SUL DB\n",
    "#=================================================================================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================================================================================================================================================================\n",
    "#                                                            SISTEMO DATI TEMPORALI\n",
    "#=================================================================================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sistemaDatiTemporali(df,MESE_RIFERIMENTO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================================================================================================================================================================\n",
    "#                                                            CREAZIONE MATRICOLE\n",
    "#=================================================================================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   tolgo colonne inutili\n",
    "df.drop('DATA_SUP',axis=1,inplace=True)\n",
    "df.drop('DATA_FREQ',axis=1,inplace=True)\n",
    "df.drop('DATA_PRIMO_APPELLO',axis=1,inplace=True)\n",
    "df.drop('AA_OFF_ID',axis=1,inplace=True)\n",
    "df.drop('PESO',axis=1,inplace=True)\n",
    "df.drop('DATA_CHIUSURA',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matricole = calcolaMatricole(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lo utilizzero' per prevedere le iscrizioni al secondo anno\n",
    "matricoleConCodice=[]\n",
    "for elem in matricole:\n",
    "    copia = elem.copy(deep=True)\n",
    "    matricoleConCodice.append(copia)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ELIMINO LE COLONNE INUTILI E CREO DB PER VISUALIZZARE GLI STUDENTI\n",
    "for elem in matricole:\n",
    "    elem.drop('STU_ID',axis=1,inplace=True)\n",
    "    elem.drop('ULTIMO_TENTATIVO',axis=1,inplace=True)\n",
    "    elem.drop('PRIMO_ANNO',axis=1,inplace=True)\n",
    "\n",
    "#creo dataframe db per la visualizzazione più comoda\n",
    "db = []\n",
    "for elem in matricole:\n",
    "    temp = elem.copy(deep=True)\n",
    "    db.append(temp)\n",
    "\n",
    "for elem in matricole:\n",
    "    elem.drop('DES',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALIZZO STRUTTURA DATI\n",
    "es = 0\n",
    "print('matricole:')\n",
    "display(matricole[es])    #   stato attuale\n",
    "print('db:')\n",
    "display(db[es])\n",
    "# print('cfu: ')\n",
    "# print(cfu[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#esporto matricole\n",
    "if ESPORTA_DATI:\n",
    "    esportaMatricole(matricole)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#esporto cfu\n",
    "if ESPORTA_DATI:\n",
    "    esportaCFU(cfu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================================================================================================================================================================\n",
    "#                                                                           CALCOLO X,Y\n",
    "#=================================================================================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcolo X\n",
    "\n",
    "X = matricole.copy()\n",
    "print('elementi X: ', len(X))\n",
    "\n",
    "#   CONVERSIONE A NUMPY ARRAY X\n",
    "#conversione X\n",
    "temp = [np.array(X[0].to_numpy().flatten())]\n",
    "for i in range(1,len(X)):\n",
    "    temp = np.append(temp,[X[i].to_numpy().flatten()],axis=0)\n",
    "\n",
    "X = temp\n",
    "\n",
    "print('tutto X:')\n",
    "print(X)\n",
    "print('-------------')\n",
    "print('singola riga X:')\n",
    "print(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creazione y\n",
    "\n",
    "#   creo array che mi dice per ciascuno studente se e' iscritto al secondo anno o meno.\n",
    "#   se lo studente ha provato almeno un esame lo considero iscritto, altrimenti no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CARICO I DATI\n",
    "\n",
    "dfOriginale = pd.read_excel('data/Informatica.xlsx')\n",
    "dfOriginale = dfOriginale[colonneUtilizzate]\n",
    "dfOriginale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matricoleConCodice[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vado a guardare classificare gli studenti tra attivi al II anno (se provano almeno un esame in tutto l' anno), oppure in inattivi\n",
    "\n",
    "attiviSecondoAnno = []\n",
    "\n",
    "#indica il numero di studenti iscritti all' anno successivo nel 2016,17,18\n",
    "# (lo uso solo per fare analisi sui dati)\n",
    "attiviNegliAnni = [0,0,0]\n",
    "\n",
    "for i in range(len(matricoleConCodice)):\n",
    "\n",
    "    #trovo id studente\n",
    "    idStudente = matricoleConCodice[i].iloc[0,0]\n",
    "    primoAnno = matricoleConCodice[i].iloc[0,4]\n",
    "\n",
    "    attivoSecondoAnno = False\n",
    "    #cerco un esame con l' id dello studente che lo studente ha provato o superato al secondo anno\n",
    "    for j in range(len(dfOriginale)):\n",
    "        if dfOriginale.loc[j,'STU_ID'] == idStudente:\n",
    "\n",
    "            annoMaxAppello = dfOriginale.loc[j,'DATA_MAX_APPELLO']\n",
    "            annoMaxAppello = str(annoMaxAppello)[0:4]\n",
    "            \n",
    "            annoSup = dfOriginale.loc[j,'DATA_SUP']\n",
    "            annoSup = str(annoSup)[0:4]\n",
    "            \n",
    "            annoPrimoApp = dfOriginale.loc[j,'DATA_PRIMO_APPELLO']\n",
    "            annoPrimoApp = str(annoPrimoApp)[0:4]\n",
    "            \n",
    "            if annoMaxAppello == str(primoAnno+1) or annoSup == str(primoAnno+1) or annoPrimoApp == str(primoAnno+1):\n",
    "                attivoSecondoAnno=True\n",
    "    \n",
    "    if attivoSecondoAnno:\n",
    "        attiviSecondoAnno.append(1)\n",
    "        if primoAnno == 2016: attiviNegliAnni[0]+=1\n",
    "        elif primoAnno == 2017: attiviNegliAnni[1]+=1\n",
    "        elif primoAnno == 2018: attiviNegliAnni[2]+=1\n",
    "    else:\n",
    "        attiviSecondoAnno.append(0)\n",
    "    \n",
    "print('studenti attivi nell\\' anno successivo nel:')\n",
    "print('2016: ',attiviNegliAnni[0])\n",
    "print('2017: ',attiviNegliAnni[1])\n",
    "print('2018: ',attiviNegliAnni[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcolo il nuovo y\n",
    "y = attiviSecondoAnno.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conversione y\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===============bilanciamento dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "somma1 = 0\n",
    "somma0 = 0\n",
    "for elem in y:\n",
    "    if elem == 1: somma1+=1\n",
    "    else: somma0 +=1\n",
    "\n",
    "perc0 = round(somma0/len(y)*100,2)\n",
    "perc1 = round((100 - perc0),2)\n",
    "print('studenti non iscritti II anno',somma0)\n",
    "print('studenti iscritti II anno: ',somma1)\n",
    "\n",
    "# Pie chart, where the slices will be ordered and plotted counter-clockwise:\n",
    "labels = 'Iscritti II anno\\n'+str(somma0)+'('+str(perc0)+'%)', 'Non iscritti II anno\\n'+str(somma1)+'('+str(perc1)+'%)'\n",
    "sizes = [somma0, somma1]\n",
    "\n",
    "fig1, ax1 = plt.subplots()\n",
    "# ax1.pie(sizes, labels=labels,startangle=90,colors=[ROSSO,VERDE],wedgeprops={\"edgecolor\":\"k\",'linewidth': 2,'antialiased': True},textprops={'fontsize': 20})\n",
    "ax1.pie(sizes,startangle=90,colors=[ROSSO,VERDE],wedgeprops={\"edgecolor\":\"k\",'linewidth': 2,'antialiased': True},textprops={'fontsize': 20})\n",
    "ax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "\n",
    "plt.savefig('generatedImages/sbilanciamentoDataset2.png', dpi=300,bbox_inches='tight',facecolor='w')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oversampling\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote = SMOTE(sampling_strategy='minority')\n",
    "X_sm, y_sm = smote.fit_resample(X,y)\n",
    "X = X_sm\n",
    "y = y_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "somma1 = 0\n",
    "somma0 = 0\n",
    "for elem in y:\n",
    "    if elem == 1: somma1+=1\n",
    "    else: somma0 +=1\n",
    "\n",
    "perc0 = round(somma0/len(y)*100,2)\n",
    "perc1 = round((100 - perc0),2)\n",
    "print('studenti inattivi II anno',somma0)\n",
    "print('studenti attivi II anno: ',somma1)\n",
    "\n",
    "# Pie chart, where the slices will be ordered and plotted counter-clockwise:\n",
    "labels = 'Iscritti II anno\\n'+str(somma0)+'('+str(perc0)+'%)', 'Non iscritti II anno\\n'+str(somma1)+'('+str(perc1)+'%)'\n",
    "sizes = [somma0, somma1]\n",
    "\n",
    "fig1, ax1 = plt.subplots()\n",
    "# ax1.pie(sizes, labels=labels,startangle=90,colors=[ROSSO,VERDE],wedgeprops={\"edgecolor\":\"k\",'linewidth': 2,'antialiased': True},textprops={'fontsize': 20})\n",
    "ax1.pie(sizes, startangle=90,colors=[ROSSO,VERDE],wedgeprops={\"edgecolor\":\"k\",'linewidth': 2,'antialiased': True},textprops={'fontsize': 20})\n",
    "ax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "\n",
    "plt.savefig('generatedImages/bilanciamentoDataset2.png', dpi=300,bbox_inches='tight',facecolor='w')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, train_size=0.75, test_size=0.25,random_state=5)\n",
    "\n",
    "#   DIMENSIONI X, y\n",
    "print('dimensioni X: ',X.shape)\n",
    "print('dimensioni y: ',y.shape)\n",
    "print('\\ndimensioni X_train: ',X_train.shape)\n",
    "print('dimensioni y_train: ',y_train.shape)\n",
    "print('dimensioni X_test: ',X_test.shape)\n",
    "print('dimensioni y_test: ',y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calcolo percentuale di studenti che super i 40 cfu in train set\n",
    "counts = [0,0]\n",
    "for i in range(len(y_train)):\n",
    "    if y_train[i]==0:counts[0]+=1\n",
    "    else:counts[1]+=1\n",
    "\n",
    "print(' percentuale di studdenti iscritti al II anno in train set: ',counts[1]/(counts[0]+counts[1])*100)\n",
    "\n",
    "#calcolo percentuale di studenti che super i 40 cfu in test set\n",
    "counts = [0,0]\n",
    "for i in range(len(y_test)):\n",
    "    if y_test[i]==0:counts[0]+=1\n",
    "    else:counts[1]+=1\n",
    "\n",
    "print(' percentuale di studdenti iscritti al II anno in test set',counts[1]/(counts[0]+counts[1])*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================================================================================================================================================================\n",
    "#                                                            \n",
    "# \n",
    "#                                                                       ricerca modello migliore\n",
    "\n",
    "\n",
    "#=================================================================================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelsRisultatiAccuracy = []\n",
    "risultatiAccuracy = []\n",
    "risultatiPrecision = []\n",
    "risultatiRecall = []\n",
    "risultatiF1 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================================================================================================================================================================\n",
    "#                                                                     RETE NEURALE\n",
    "#=================================================================================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalizzazione input\n",
    "Xnorm = tf.keras.utils.normalize(X, axis=1)\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(28, activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(2, activation=tf.nn.softmax))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#analisi prestazioni\n",
    "acc, prec, recall, f1 = analizza(0,Xnorm,y,reteNeurale = True,nloop=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================================================================================================================================================================\n",
    "#                                                                     TPOT CLASSIFICAZIONE\n",
    "#=================================================================================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Internally, TPOT uses joblib to fit estimators in parallel. This is the same parallelization framework used by scikit-learn. \n",
    "#   But it may crash/freeze with n_jobs > 1 under OSX or Linux as scikit-learn does, especially with large datasets.\n",
    "#   One solution is to configure Python's multiprocessing module to use the forkserver start method \n",
    "#   (instead of the default fork) to manage the process pools. You can enable the forkserver mode globally for your program by putting \n",
    "#   the following codes into your main script:\n",
    "\n",
    "if USE_TPOT:\n",
    "    import multiprocessing\n",
    "    if __name__ == '__main__':\n",
    "        # multiprocessing.set_start_method('forkserver', force=True)\n",
    "\n",
    "        #   create an instance of TPOT (n_jobs = -1 utilizza tutti i core disponibili)\n",
    "        # tpot =  TPOTClassifier(generations = 100, population_size = 100, n_jobs = -1)                             #default                                                                      #impostazioni di default\n",
    "        # tpot =  TPOTClassifier(generations=2, population_size=20, verbosity=2,n_jobs = -1, random_state=50)       #video \n",
    "\n",
    "        tpot =  TPOTClassifier(generations = GENERATIONS, population_size = POP_SIZE, n_jobs = -1)  \n",
    "\n",
    "    #   You can tell TPOT to optimize a pipeline based on a data set with the fit function\n",
    "    #   The fit function initializes the genetic programming algorithm to find the highest-scoring \n",
    "    #   pipeline based on average k-fold cross-validation. Then, the pipeline is trained on the entire\n",
    "    #   set of provided samples, and the TPOT instance can be used as a fitted model.\n",
    "        tpot.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_TPOT:\n",
    "    #   You can then proceed to evaluate the final pipeline on the testing set with the score function:\n",
    "    # print(tpot.score(X_test, y_test))\n",
    "    \n",
    "    \n",
    "    # analizza(tpot)\n",
    "\n",
    "    print('il migliore modello che ho trovato è: \\n')\n",
    "    print(tpot.fitted_pipeline_)\n",
    "\n",
    "    #   Finally, you can tell TPOT to export the corresponding Python code for the optimized pipeline to a text file with the export function:\n",
    "    #   Once this code finishes running, tpot_exported_pipeline.py will contain the Python code for the optimized pipeline.\n",
    "\n",
    "    tpot.export('exported_pipeline_classification2.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================================================================================================================================================================\n",
    "#                                             gradient boosting classifier (miglior modello trovato)\n",
    "#=================================================================================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.feature_selection import SelectFwe, f_classif\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "from sklearn.svm import LinearSVC\n",
    "from tpot.builtins import StackingEstimator\n",
    "\n",
    "model = make_pipeline(\n",
    "    SelectFwe(score_func=f_classif, alpha=0.023),\n",
    "    StackingEstimator(estimator=LinearSVC(C=15.0, dual=False, loss=\"squared_hinge\", penalty=\"l1\", tol=0.01)),\n",
    "    GradientBoostingClassifier(learning_rate=0.5, max_depth=8, max_features=0.1, min_samples_leaf=3, min_samples_split=11, n_estimators=100, subsample=0.6500000000000001)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc, prec, recall, f1 = analizza(model,X,y)\n",
    "risultatiAccuracy.append(acc)\n",
    "risultatiPrecision.append(prec)\n",
    "risultatiRecall.append(recall)\n",
    "risultatiF1.append(f1)\n",
    "labelsRisultatiAccuracy.append('gradient boosting classifier')\n",
    "m=matrix(model,X_train,y_train,X_test,y_test,regression=False,tipoPrevisioneCFU=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# risultati = importanzaFeaturesMedie(10,model,X,y,regression=False)\n",
    "# risultati[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================================================================================================================================================================\n",
    "#                                             random forest classifier pipeline (trovata da tpot per questo problema)\n",
    "\n",
    "                                                        \n",
    "#=================================================================================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler\n",
    "from tpot.builtins import ZeroCount\n",
    "\n",
    "# Average CV score on the training set was: 0.8868686868686868``\n",
    "model = make_pipeline(\n",
    "    RobustScaler(),\n",
    "    ZeroCount(),\n",
    "    MinMaxScaler(),\n",
    "    RandomForestClassifier(bootstrap=True, criterion=\"entropy\", max_features=0.3, min_samples_leaf=12, min_samples_split=13, n_estimators=100)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc, prec, recall, f1 = analizza(model,X,y)\n",
    "risultatiAccuracy.append(acc)\n",
    "risultatiPrecision.append(prec)\n",
    "risultatiRecall.append(recall)\n",
    "risultatiF1.append(f1)\n",
    "labelsRisultatiAccuracy.append('random forest classifier pipeline')\n",
    "m=matrix(model,X_train,y_train,X_test,y_test,regression=False,tipoPrevisioneCFU=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importanzaFeaturesMedie(2,model,X,y,regression=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANALISI IMPORTANZA FEATURES\n",
    "# importanze, importanzePos, importanzeNeg = importanzaFeaturesMedie(10,modello,X,y,regression=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================================================================================================================================================================\n",
    "#                                                                           RANDOM FOREST\n",
    "#=================================================================================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "training_features = X_train\n",
    "testing_features = X_test\n",
    "training_target = y_train\n",
    "testing_target = y_test\n",
    "\n",
    "# Average CV score on the training set was: 0.8806451612903226\n",
    "model = RandomForestClassifier(bootstrap=False, criterion=\"gini\", max_features=0.3, min_samples_leaf=17, min_samples_split=5, n_estimators=100)\n",
    "# Fix random state in exported estimator\n",
    "if hasattr(model, 'random_state'):\n",
    "    setattr(model, 'random_state', 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc, prec, recall, f1 = analizza(model,X,y)\n",
    "risultatiAccuracy.append(acc)\n",
    "risultatiPrecision.append(prec)\n",
    "risultatiRecall.append(recall)\n",
    "risultatiF1.append(f1)\n",
    "labelsRisultatiAccuracy.append('random forest')\n",
    "m=matrix(model,X_train,y_train,X_test,y_test,regression=False,tipoPrevisioneCFU=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================================================================================================================================================================\n",
    "#                                                                           DECISION TREE\n",
    "#==================================================================================================================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   dato che non fornisco alcuna indicazione sulla massima profondità dell' albero, avremo probabilmente la massima profondita' possibile\n",
    "#   di conseguenza andremo probabilment incontro ad overfitting\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   cos_complexity_pruning = funzione che mi restituisce due valori: alphas ed impurities\n",
    "path = clf.cost_complexity_pruning_path(X_train, y_train)\n",
    "ccp_alphas, impurities = path.ccp_alphas, path.impurities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   la funzione ci ha restituito questi valori, che corrispondono ai \"weak points\" rispetto alle foglie\n",
    "#   questi valori sono un parametro che posso passare al metodo che definisce il decision tree\n",
    "ccp_alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   per ogni alpha che ho calcolato, addestro un albero, e lo salvo nell' array di alberi \"clfs\"\n",
    "clfs = []\n",
    "for ccp_alpha in ccp_alphas:\n",
    "    clf = DecisionTreeClassifier(random_state=0, ccp_alpha=ccp_alpha)\n",
    "    clf.fit(X_train, y_train)\n",
    "    clfs.append(clf)\n",
    "print(\"Number of nodes in the last tree is: {} with ccp_alpha: {}\".format(\n",
    "      clfs[-1].tree_.node_count, ccp_alphas[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#       per ogni albero che ho costruito, calcolo il suo score\n",
    "#       in questo modo riesco a calcolare il valore di alpha ottimale\n",
    "#       dove gli score sul train set sono molto alti, ho probabilmente overfitting\n",
    "#       il punto migliore è quello con il massimo risultato sul test set ed un valore piu o meno simile per il train\n",
    "\n",
    "train_scores = [clf.score(X_train, y_train) for clf in clfs]\n",
    "test_scores = [clf.score(X_test, y_test) for clf in clfs]\n",
    "\n",
    "#       visione ampia\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel(\"alpha\")\n",
    "ax.set_ylabel(\"accuracy\")\n",
    "ax.set_title(\"Accuracy vs alpha for training and testing sets\")\n",
    "ax.plot(ccp_alphas, train_scores, marker='o', label=\"train\",\n",
    "        drawstyle=\"steps-post\")\n",
    "ax.plot(ccp_alphas, test_scores, marker='o', label=\"test\",\n",
    "        drawstyle=\"steps-post\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   zoom\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel(\"alpha\")\n",
    "ax.set_ylabel(\"accuracy\")\n",
    "ax.set_title(\"Accuracy vs alpha for training and testing sets\")\n",
    "ax.plot(ccp_alphas, train_scores, marker='o', label=\"train\",\n",
    "        drawstyle=\"steps-post\")\n",
    "ax.plot(ccp_alphas, test_scores, marker='o', label=\"test\",\n",
    "        drawstyle=\"steps-post\")\n",
    "plt.xlim([0.005, 0.05])\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST SPECIFICO SUGLI ALPHA NEL RANGE PIU' INTERESSANTE\n",
    "\n",
    "#0100\n",
    "clf = DecisionTreeClassifier(random_state=0, ccp_alpha=0.0100)\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "pred=clf.predict(X_test)\n",
    "acc_0100 = accuracy_score(y_test, pred)\n",
    "\n",
    "#0125\n",
    "clf = DecisionTreeClassifier(random_state=0, ccp_alpha=0.0125)\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "pred=clf.predict(X_test)\n",
    "acc_0125 = accuracy_score(y_test, pred)\n",
    "\n",
    "#0150\n",
    "clf = DecisionTreeClassifier(random_state=0, ccp_alpha=0.0150)\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "pred=clf.predict(X_test)\n",
    "acc_0150 = accuracy_score(y_test, pred)\n",
    "\n",
    "#0175\n",
    "clf = DecisionTreeClassifier(random_state=0, ccp_alpha=0.0175)\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "pred=clf.predict(X_test)\n",
    "acc_0175 = accuracy_score(y_test, pred)\n",
    "\n",
    "#020\n",
    "clf = DecisionTreeClassifier(random_state=0, ccp_alpha=0.020)\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "pred=clf.predict(X_test)\n",
    "acc_0200 = accuracy_score(y_test, pred)\n",
    "\n",
    "print('acc 01: ',acc_0100)\n",
    "print('acc 0125: ',acc_0125)\n",
    "print('acc 0150: ',acc_0150)\n",
    "print('acc 0175: ',acc_0175)\n",
    "print('acc 02: ',acc_0200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   ADDESTRAMENTO ALBERO PRUNED\n",
    "# model = DecisionTreeClassifier(random_state=0, ccp_alpha=0.02)\n",
    "model = DecisionTreeClassifier(random_state=0, ccp_alpha=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc, prec, recall, f1 = analizza(model,X,y)\n",
    "risultatiAccuracy.append(acc)\n",
    "risultatiPrecision.append(prec)\n",
    "risultatiRecall.append(recall)\n",
    "risultatiF1.append(f1)\n",
    "labelsRisultatiAccuracy.append('decision tree')\n",
    "m=matrix(model,X_train,y_train,X_test,y_test,regression=False,tipoPrevisioneCFU=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   stampo albero potato\n",
    "\n",
    "from sklearn import tree\n",
    "plt.figure(figsize=(15,10))\n",
    "tree.plot_tree(clf,filled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================================================================================================================================================================\n",
    "#                                                                           SVC\n",
    "#=================================================================================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#addestramento\n",
    "from sklearn.svm import SVC\n",
    "model = SVC(gamma='auto', probability=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc, prec, recall, f1 = analizza(model,X,y)\n",
    "risultatiAccuracy.append(acc)\n",
    "risultatiPrecision.append(prec)\n",
    "risultatiRecall.append(recall)\n",
    "risultatiF1.append(f1)\n",
    "labelsRisultatiAccuracy.append('SVC')\n",
    "m=matrix(model,X_train,y_train,X_test,y_test,regression=False,tipoPrevisioneCFU=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================================================================================================================================================================\n",
    "#                                                                           NAIVE BAYES\n",
    "#=================================================================================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   DEFINIZIONE MODELLO\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "model = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc, prec, recall, f1 = analizza(model,X,y)\n",
    "risultatiAccuracy.append(acc)\n",
    "risultatiPrecision.append(prec)\n",
    "risultatiRecall.append(recall)\n",
    "risultatiF1.append(f1)\n",
    "labelsRisultatiAccuracy.append('naive bayes')\n",
    "m=matrix(model,X_train,y_train,X_test,y_test,regression=False,tipoPrevisioneCFU=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================================================================================================================================================================\n",
    "#                                                                     NEAREST NEIGHBOUR\n",
    "#=================================================================================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "model = KNeighborsClassifier(n_neighbors=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc, prec, recall, f1 = analizza(model,X,y)\n",
    "risultatiAccuracy.append(acc)\n",
    "risultatiPrecision.append(prec)\n",
    "risultatiRecall.append(recall)\n",
    "risultatiF1.append(f1)\n",
    "labelsRisultatiAccuracy.append('nearest neighbour')\n",
    "m=matrix(model,X_train,y_train,X_test,y_test,regression=False,tipoPrevisioneCFU=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================================================================================================================================================================\n",
    "#                                                                  SGD\n",
    "#=================================================================================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "model = SGDClassifier(loss=\"hinge\", penalty=\"l2\", max_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc, prec, recall, f1 = analizza(model,X,y)\n",
    "risultatiAccuracy.append(acc)\n",
    "risultatiPrecision.append(prec)\n",
    "risultatiRecall.append(recall)\n",
    "risultatiF1.append(f1)\n",
    "labelsRisultatiAccuracy.append('SGD')\n",
    "m=matrix(model,X_train,y_train,X_test,y_test,regression=False,tipoPrevisioneCFU=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================================================================================================================================================================\n",
    "#                                                                    \n",
    "# \n",
    "# \n",
    "#                                                       REGRESSIONE\n",
    "#\n",
    "#\n",
    "#\n",
    "#=================================================================================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================================================================================================================================================================\n",
    "#                                                                           TPOT REGRESSIONE\n",
    "#=================================================================================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Internally, TPOT uses joblib to fit estimators in parallel. This is the same parallelization framework used by scikit-learn. \n",
    "#   But it may crash/freeze with n_jobs > 1 under OSX or Linux as scikit-learn does, especially with large datasets.\n",
    "#   One solution is to configure Python's multiprocessing module to use the forkserver start method \n",
    "#   (instead of the default fork) to manage the process pools. You can enable the forkserver mode globally for your program by putting \n",
    "#   the following codes into your main script:\n",
    "\n",
    "if USE_TPOT:\n",
    "    from tpot import TPOTRegressor\n",
    "    import multiprocessing\n",
    "    if __name__ == '__main__':\n",
    "        multiprocessing.set_start_method('forkserver', force=True)\n",
    "\n",
    "        #   create an instance of TPOT (n_jobs = -1 utilizza tutti i core disponibili)\n",
    "        # tpot =  TPOTClassifier(generations = 100, population_size = 100, n_jobs = -1)                             #default                                                                      #impostazioni di default\n",
    "        # tpot =  TPOTClassifier(generations=2, population_size=20, verbosity=2,n_jobs = -1, random_state=50)       #video \n",
    "\n",
    "        tpot =  TPOTRegressor(generations = 80, population_size = 80, n_jobs = -1)  \n",
    "\n",
    "    #   You can tell TPOT to optimize a pipeline based on a data set with the fit function\n",
    "    #   The fit function initializes the genetic programming algorithm to find the highest-scoring \n",
    "    #   pipeline based on average k-fold cross-validation. Then, the pipeline is trained on the entire\n",
    "    #   set of provided samples, and the TPOT instance can be used as a fitted model.\n",
    "        tpot.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_TPOT:\n",
    "    print('il migliore modello che ho trovato è: \\n')\n",
    "    print(tpot.fitted_pipeline_)\n",
    "\n",
    "    #   Finally, you can tell TPOT to export the corresponding Python code for the optimized pipeline to a text file with the export function:\n",
    "    #   Once this code finishes running, tpot_exported_pipeline.py will contain the Python code for the optimized pipeline.\n",
    "\n",
    "    tpot.export('exported_pipeline_regression2.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================================================================================================================================================================\n",
    "#                                                            REGRESSIONE LINEARE\n",
    "#=================================================================================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "model = linear_model.LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc, prec, recall, f1 = analizzaRegr(model,X,y)\n",
    "risultatiAccuracy.append(acc)\n",
    "risultatiPrecision.append(prec)\n",
    "risultatiRecall.append(recall)\n",
    "risultatiF1.append(f1)\n",
    "labelsRisultatiAccuracy.append('linear regression')\n",
    "m=matrix(model,X_train,y_train,X_test,y_test,regression=True,tipoPrevisioneCFU=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================================================================================================================================================================\n",
    "#                                                            REGRESSIONE LOGISTICA\n",
    "#=================================================================================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc, prec, recall, f1 = analizzaRegr(model,X,y)\n",
    "risultatiAccuracy.append(acc)\n",
    "risultatiPrecision.append(prec)\n",
    "risultatiRecall.append(recall)\n",
    "risultatiF1.append(f1)\n",
    "labelsRisultatiAccuracy.append('logistic regression')\n",
    "m=matrix(model,X_train,y_train,X_test,y_test,regression=True,tipoPrevisioneCFU=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================================================================================================================================================================\n",
    "#                                                            Extra tree regressor\n",
    "#=================================================================================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import ExtraTreesRegressor, RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from tpot.builtins import StackingEstimator, ZeroCount\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from copy import copy\n",
    "\n",
    "# Average CV score on the training set was: -0.10277989254463067\n",
    "model = make_pipeline(\n",
    "    make_union(\n",
    "        RobustScaler(),\n",
    "        FunctionTransformer(copy)\n",
    "    ),\n",
    "    StackingEstimator(estimator=ExtraTreesRegressor(bootstrap=False, max_features=0.9000000000000001, min_samples_leaf=17, min_samples_split=4, n_estimators=100)),\n",
    "    ZeroCount(),\n",
    "    StackingEstimator(estimator=RandomForestRegressor(bootstrap=True, max_features=0.05, min_samples_leaf=2, min_samples_split=19, n_estimators=100)),\n",
    "    ExtraTreesRegressor(bootstrap=False, max_features=0.7500000000000001, min_samples_leaf=9, min_samples_split=19, n_estimators=100)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc, prec, recall, f1 = analizzaRegr(model,X,y)\n",
    "risultatiAccuracy.append(acc)\n",
    "risultatiPrecision.append(prec)\n",
    "risultatiRecall.append(recall)\n",
    "risultatiF1.append(f1)\n",
    "labelsRisultatiAccuracy.append('extra tree regressor')\n",
    "m=matrix(model,X_train,y_train,X_test,y_test,regression=True,tipoPrevisioneCFU=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=====================================================================================\n",
    "\n",
    "#====================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labelsRisultatiAccuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #scrivo i risultati sul file \"performance 1\"\n",
    "# with open('performance2.txt', 'a') as f:\n",
    "#     f.write(\"%s\\n\" % MESE_RIFERIMENTO)\n",
    "#     for item in risultatiAccuracy:\n",
    "#         f.write(\"%s \" % item)\n",
    "#     f.write('\\n')\n",
    "#     for item in risultatiPrecision:\n",
    "#         f.write(\"%s \" % item)\n",
    "#     f.write('\\n')\n",
    "#     for item in risultatiRecall:\n",
    "#         f.write(\"%s \" % item)\n",
    "#     f.write('\\n')\n",
    "#     for item in risultatiF1:\n",
    "#         f.write(\"%s \" % item)\n",
    "#     f.write('\\n')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "13bf5a2e2f57d04bc926dca3f2f92356a462738c3244a3c0db52bdbeb393fd7e"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('tensor': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
